<!-- AUTO-GENERATED by bot/scripts/project_sync.py -->
<!-- Last updated: 2025-10-04T00:00:00Z -->
<!-- DO NOT EDIT MANUALLY - Changes will be overwritten -->

# Project: TJA Analysis

## Classification

- **Type**: Analysis (Academic Paper)
- **Status**: Active
- **Priority**: P1 (linked to automod)
- **Goal**: Academic Profile, Accountability

## Quick Facts

- **Location**: `papers/automod/tja/`
- **Strategic Doc**: Referenced in `data/projects/automod-demo.md`
- **Tech Stack**: dbt, BigQuery, Looker Studio, Python/Buttermilk
- **Dependencies**: buttermilk, automod
- **Purpose**: Evaluate AI content moderation of trans-related news

## Purpose

Academic research collaboration with Trans Journalists Association (TJA):

- Evaluate AI ability to apply TJA content moderation guidelines
- Compare against other criteria (GLAAD, HRC, Australian law)
- Assess AI accuracy vs human-coded golden set
- Publication on content moderation effectiveness

## Architecture

**Research Pipeline**:

```
Golden Set (28 hand-coded articles)
  ↓
Buttermilk Flows (JUDGE→SYNTH→SCORER)
  ├── Multiple LLMs evaluate each article
  ├── Apply TJA/GLAAD/HRC criteria
  └── SCORER agents evaluate accuracy
  ↓
BigQuery (prosocial-443205.testing.flows)
  ├── AgentTrace records
  └── SCORER evaluations
  ↓
dbt Models (papers/automod/tja/dbt/)
  ├── fct_judge_predictions
  ├── fct_synth_predictions
  ├── fct_scores
  └── tja_judge_comparison (A/B testing)
  ↓
Looker Studio Dashboard
  ↓
Paper Analysis
```

**Data Architecture**:

- **Golden Set**: 28 articles with criterion-referenced human coding
- **Predictions**: JUDGE and SYNTH agent outputs
- **Scores**: Multiple SCORER agents evaluate each prediction
- **Experiments**: A/B tests on prompt templates (template hashes)

## Data Sources

**BigQuery Table**: `prosocial-443205.testing.flows`

- AgentTrace objects (highly normalized)
- Joined/unnested in analysis views
- Individual predictions may appear as multiple rows (distinct on call_id)

**Experiment Tracking**:

- Config hash: Configuration file hash
- Template hash: System prompt hash
- Record hash: Golden set item hash
- Filters out test/debug runs

**Key Fields**:

- `experiment_group`: e.g., "JUDGE - tja", "SYNTH - glaad"
- `experiment_criteria`: Which guidelines (tja/glaad/hrc/australian)
- `call_id`: Unique prediction identifier
- Qualitative and quantitative ratings from SCORERs

## Development

**dbt Workflow**:

```bash
cd papers/automod/tja/dbt
dbt run      # Execute transformation models
dbt test     # Validate data quality
dbt docs generate && dbt docs serve
```

**Run Experiments**:

```bash
# Add new YAML config to automod.cc/conf/
# Results automatically logged to BigQuery
# No code changes needed
```

**Dashboard**: Streamlit dashboard (locally hosted, pulls data via dbt models)

**Spreadsheet**: https://docs.google.com/spreadsheets/d/1N98c28IZE9xjvAUp2vLE8cIR6E7qoq7aFUyi0dLrSMU

## Impact Analysis

### Changes Here Affect

**Stakeholders**:

- TJA (Trans Journalists Association) - research partner
- Academic peer reviewers
- Content moderation policy community
- GLAAD (related stakeholder)

**Risk Level**: MEDIUM

- Academic publication standards
- Research reproducibility requirements
- Stakeholder relationship (TJA)

### Changes to Dependencies Affect This

**Buttermilk Changes**:

- **Flow execution**: May affect prediction generation
- **AgentTrace schema**: CRITICAL - breaks entire pipeline
- **LLM integration**: May affect prediction quality

**Automod Changes**:

- **Config system**: May affect experiment definitions
- **JUDGE/SYNTH/SCORER agents**: May affect evaluation pipeline

**dbt Changes**:

- **Model schema**: Update downstream analysis
- **BigQuery views**: Update dashboard queries
- **Test definitions**: Revalidate data quality

**Protocol** (from CROSS_CUTTING_CONCERNS.md):

1. **Never modify golden set** without documentation
2. **New experiments**: Add YAML configs, not code
3. **Analysis updates**: Use dbt for transformations
4. **Dashboard updates**: After data schema changes

## Critical Data Rules

**Golden Set (28 Articles)**:

- NEVER modify without explicit documentation
- Source of truth for evaluation
- Contains satire examples
- Hand-coded by domain experts (Lucinda validated/rewrote)

**BigQuery Data**:

- NEVER manually edit
- Source of truth for all predictions
- Highly normalized (join carefully)
- Distinct on call_id for aggregations

**dbt Models**:

- Source of truth for transformations
- Version before breaking changes
- Document all columns in schema.yml
- Test data quality

## Experiment Types

**Standard Evaluation**:

- Each article evaluated by multiple LLMs
- Criteria: TJA, GLAAD, HRC, Australian
- Role: JUDGE (1-shot) and SYNTH (meta-analysis)
- Stochastic: 10+ runs per prediction

**A/B Testing**:

- Compare different prompt templates
- Template hash tracks versions
- Example: "JUDGE - tja" with template variations
- Analysis models: `tja_judge_comparison.sql`

## Recent Activity

<!-- AUTO-POPULATED by project_sync.py -->

### 2025-08

- Lucinda validated and rewrote golden answers
- Prompt difference check implemented (#189)
- TJA price estimate bug fixed (#190)

### Previous

- Full evaluation pipeline operational
- dbt models for comparative analysis
- Dashboard production-ready
- Multiple criteria evaluations complete

## Current State

**Golden Set**: 28 articles (validated by Lucinda) **Evaluations**: Multiple runs completed **dbt Models**: Transformation pipeline operational **Dashboard**: Production analysis views active **Paper**: Analysis in progress

## Critical Reminders for Agents

1. **Golden set is sacred** - document any changes
2. **BigQuery is read-only** - never manual edits
3. **dbt for all analysis** - don't query BigQuery directly
4. **Version dbt models** - before breaking changes
5. **Dashboard** - update after schema changes
6. **New experiments** - YAML configs in automod.cc/conf/
7. **Distinct on call_id** - when aggregating predictions
8. **Template hashing** - tracks A/B test versions
9. **Update strategic doc** via `project_sync.py`

## Analysis Guidance

**Filtering Experiments**:

```sql
-- Example: TJA Judge comparison
WHERE experiment_group = 'JUDGE - tja'
  AND template_hash IN ('hash1', 'hash2')
```

**Aggregating Predictions**:

```sql
-- MUST use distinct call_id
SELECT DISTINCT call_id, prediction, score
FROM predictions_view
```

**Comparing Criteria**:

```sql
-- Different guidelines on same articles
WHERE experiment_criteria IN ('tja', 'glaad', 'hrc')
```

## Related Documentation

- Strategic context: `data/projects/automod-demo.md`
- Automod technical: `docs/projects/automod.md`
- dbt documentation: `papers/automod/tja/dbt/` (run `dbt docs serve`)
- Experiment filtering guide: `papers/automod/tja/docs/Experiment_Filtering_Guide.md`
- Dashboard guide: `papers/automod/tja/docs/TJA_Template_Analysis_Dashboard_Guide.md`
- Cross-cutting concerns: `docs/CROSS_CUTTING_CONCERNS.md`
