<!-- AUTO-GENERATED by bot/scripts/project_sync.py -->
<!-- Last updated: 2025-10-04T00:00:00Z -->
<!-- DO NOT EDIT MANUALLY - Changes will be overwritten -->

# Project: Automod (automod.cc)

## Classification

- **Type**: Research Application
- **Status**: Production Demo
- **Priority**: P1 (High-risk, high-reward)
- **Goals**: Accountability, Academic Profile

## Quick Facts

- **Location**: `projects/automod.cc/`
- **Strategic Doc**: `data/projects/automod-demo.md`
- **Tech Stack**:
  - Backend: Python, Buttermilk
  - Frontend: SvelteKit, Bootstrap, HTMX, WebSockets
  - Deploy: Cloudflare Workers, Google Cloud Run
  - Data: BigQuery (`prosocial-443205.testing.flows`), W&B
- **Dependencies**: buttermilk (flows, agents, orchestration)
- **Deployment**: Cloudflare + GCP

## Purpose

Public demo of AI content moderation for platform accountability:

- Prove Buttermilk platform capabilities
- Practical accountability tool for stakeholders (GLAAD)
- Research platform for evaluating AI moderation
- Teaching tool for content moderation issues

**Strategic Focus** (Aug 2025): "Proving and selling" the working demo, not further technical development.

## Architecture

**UI Paradigm**: Retro terminal IRC-style chat interface

- User supervises AI agents in group chat format
- Terminal aesthetics (classic IRC client style)
- Real-time WebSocket communication
- Researcher maintains control

**Evaluation Pipeline (JUDGE → SYNTH)**:

```
1. JUDGE Round
   Multiple LLMs (Gemini, Claude, GPT-4) independently evaluate content
   Each applies same guidelines (GLAAD/TJA/Australian/HRC)

2. SYNTH Round
   Meta-analyst reviews JUDGE answers
   Produces robust synthesized decision

3. SCORER Agents
   Evaluate predictions against golden set
   Multiple LLMs score accuracy

4. Stochastic Testing
   10+ runs per prediction for stability

5. Analysis
   BigQuery → dbt models → Streamlit dashboard
   Dashboard: Locally hosted Streamlit app
```

**Data Architecture**:

```
Buttermilk Flows (config in /conf)
  ↓
AgentTrace records → BigQuery
  ↓
dbt transformations
  ↓
Streamlit dashboard + Google Sheets
  ↓
Academic papers
```

## Development

**Backend**:

```bash
cd projects/automod.cc
pip install uv
uv install
pytest  # Run buttermilk flow tests
```

**Frontend**:

```bash
cd projects/automod.cc/modbot
npm install
npm run dev  # Development server
npm run build  # Production build
npm run deploy  # Deploy to Cloudflare
```

**Configuration**:

- Flows defined in YAML: `projects/automod.cc/conf/`
- No code changes needed for new experiments
- Add YAML configs to test new prompts/models/criteria

## Sub-Projects

### TJA Analysis

- **Location**: `papers/automod/tja/`
- **Purpose**: Trans-related news content moderation research
- **Dataset**: 28 hand-coded articles (golden set)
- **Analysis**: dbt models in `papers/automod/tja/dbt/`
- **See**: `docs/projects/tja.md` for details

## Impact Analysis

### Changes Here Affect

**Stakeholders**:

- GLAAD (Jenni) - primary stakeholder
- DIGI conference attendees
- Students (teaching integration)
- Academic publications

**Risk Level**: HIGH

- Production demo for key stakeholders
- DIGI keynote showcase (Sept 2025)
- Academic credibility depends on rigor

### Changes to Dependencies Affect This

**Buttermilk Changes**:

- **Flow API**: May break evaluation pipeline
- **Agent Framework**: Retest JUDGE/SYNTH/SCORER flows
- **BigQuery Logging**: Critical - all results tracked here
- **Configuration Schema**: Update YAML configs

**Protocol**:

1. Buttermilk changes → run `pytest tests/integration/buttermilk_*.py`
2. Any failures → HALT
3. Configuration changes → test flow execution
4. Success → update strategic doc

**dbt Model Changes**:

- See `docs/CROSS_CUTTING_CONCERNS.md` → "dbt Model Changes"
- Version models before breaking changes
- Update Streamlit dashboard
- Document in changelog

## Recent Activity

<!-- AUTO-POPULATED by project_sync.py -->

### 2025-09

- DIGI keynote delivered (Sept 18)
- Testing server deployed (Sept 1)
- Lesson learned: Unclear pitch narrative (see strategic doc)

### 2025-08

- TJA price estimate bug fixed (#190)
- Prompt difference check implemented (#189)
- Lucinda validated/rewrote golden answers

### Previous

- Full JUDGE→SYNTH→SCORER pipeline operational
- Streamlit dashboard production-ready
- Multiple criteria supported (GLAAD/TJA/HRC/Australian)

## Current State

**Production**: Demo operational at automod.cc **Frontend**: SvelteKit terminal interface deployed **Backend**: Buttermilk flows on GCP **Data**: BigQuery + W&B + Streamlit **Strategic Phase**: "Proving and selling" to GLAAD

**Open Questions** (Strategic):

- GLAAD needs hands-on tool OR media monitor?
- GenAI output evaluator for engineers?
- Next steps pending GLAAD feedback

## Roadmap

### Horizon 1: Self-Service Runs (Immediate) - Issue #217

Enable non-programmers (Sasha) to run experiments

- **No code changes needed** - config already exposed to frontend
- Load flows from `/conf` directory

### Horizon 2: Automated Evaluations (Medium) - Issue #216

One-click full evaluation across all examples/models

- Design `run_full_evaluation` backend flow
- Trigger from web interface

### Horizon 3: Interactive Tinkering (Strategic) - Issue #215

Low-friction prompt experimentation

- Research commercial multi-LLM testing tools
- Strategic decision on implementation

## Critical Reminders for Agents

1. **Production demo** - test thoroughly before changes
2. **GLAAD stakeholder** - coordinate major changes
3. **BigQuery is source of truth** - never manually edit data
4. **dbt models** - use for all analysis, version before breaking changes
5. **New experiments** - add YAML configs, not code
6. **Dashboard** - update after data schema changes
7. **Update strategic doc** after milestones via `project_sync.py`

## Related Documentation

- Strategic context: `data/projects/automod-demo.md`
- Frontend instructions: `projects/automod.cc/modbot/CLAUDE.md`
- TJA analysis: `docs/projects/tja.md`
- Buttermilk flows: `projects/buttermilk/docs/bots/FLOWS.md`
- Cross-cutting concerns: `docs/CROSS_CUTTING_CONCERNS.md`
