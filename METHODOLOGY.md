# Methodology: OSB LLM Benchmarking for Content Moderation

## Research Questions

1. **Primary**: How do leading large language models respond when prompted to generate content that intersects with established content moderation boundaries?
2. Do LLMs apply content moderation rules consistently across different categories of protected expression (political speech, blasphemy, counter-terrorism, historical facts)?
3. Where do LLM refusal patterns diverge from Oversight Board decisions on similar content?
4. How do prompt variations (target, claim, format) affect model willingness to generate potentially problematic content?

## Theoretical Framework

This work builds on the intersection of two bodies of literature:

- **Content moderation governance**: The Meta Oversight Board provides expert human decisions grounded in international human rights law (particularly the UN Human Rights Committee General Comment 34 on Article 19 ICCPR). These decisions serve as a normative benchmark for what constitutes legitimate versus impermissible content restrictions.
- **LLM safety alignment**: Commercial LLMs embed content policies through training and system-level guardrails. These policies may over- or under-restrict content relative to human rights standards, creating measurable divergences from expert human judgment.

The core insight is that OSB decisions represent a calibrated, expert-reasoned position on where the line between protected and harmful expression lies. By prompting LLMs to generate content in these boundary areas, we can measure where model safety policies diverge from human rights-informed expert decisions.

## Research Design

**Design type**: Comparative observational study with repeated measures across models.

We submit identical content generation prompts to multiple LLMs and compare their responses against Oversight Board decisions on analogous content. This is not a binary policy classification task ("does this violate policy X?") but a content generation task that tests model willingness to produce content in areas where the OSB has ruled expression should be protected.

### Variables

**Dependent variable(s)**:

- Model response: compliance (generates content), refusal (declines), or partial compliance (generates with caveats/modifications)
- Response characteristics: tone, completeness, hedging language, disclaimer presence

**Independent variable(s)**:

- Model identity (Gemini 3 Flash, Claude 4.5 Sonnet, GPT 5.2 Mini)
- Content moderation category (political criticism/lese majesty, government criticism, counter-terrorism expression, blasphemy, historical facts)
- Prompt target and claim specifics

**Control variables**:

- Prompt template held constant across models (only target and claim vary)
- Single-shot prompting (no multi-turn escalation)
- Default model parameters (temperature, system prompts as provided by each API)

### Unit of Analysis

A single prompt-model pair: one content generation prompt submitted to one LLM, yielding one response.

## Data Sources

**Test cases**: Derived from Oversight Board decisions and UN Human Rights Committee General Comment 34 categories (see parent task for specific GC 34 excerpts on paras 38, 43, 46, 48, 49). Initial set of 10 cases provided by Elsa (OSB), with potential expansion to ~20 cases from published OSB decisions.

**Model responses**: Collected programmatically via API calls (see "Significance of API Access" below).

## Measurement Strategy

### Construct: Model Alignment with Human Rights Standards

- **Operationalization**: For each test case, the OSB has ruled that the relevant expression should be protected. A model that refuses to generate analogous content is _more restrictive_ than the human rights standard. A model that complies is _aligned_ with the standard.
- **Measurement**: Categorical classification of each response (comply/refuse/partial), supplemented by structured capture of response metadata (latency, token count, refusal language).

## Analysis Approach

- Descriptive comparison: response distribution (comply/refuse/partial) by model and by content category
- Cross-model divergence: where models disagree with each other and with OSB positions
- Category analysis: which content moderation categories produce the most model refusals relative to OSB permissions

See `methods/` directory for implementation details (when created).

## Significance of API Access

API access to LLMs is methodologically critical for this study, distinct from using web-based chat interfaces, for several reasons:

1. **Reproducibility**: API calls with fixed parameters produce deterministic conditions. Web interfaces may inject system prompts, apply A/B testing, or modify behavior based on user history, account type, or geographic location. API access provides a controlled, auditable invocation path.

2. **Structured data capture**: API responses include machine-readable metadata (token counts, latency, finish reason, model version identifiers) that web interfaces do not expose. This metadata is essential for systematic comparison across models and for detecting refusal mechanisms (e.g., distinguishing a safety filter stop from a completed generation).

3. **Batch execution**: Submitting identical prompts to three models across multiple test cases requires programmatic access. Manual submission through web interfaces introduces human timing variation, copy-paste errors, and makes the study impractical to scale beyond a handful of cases.

4. **Parameter control**: APIs expose generation parameters (temperature, max tokens, system prompt) that web interfaces abstract away. Holding these constant across runs is necessary for valid cross-model comparison.

5. **Provenance and auditability**: Each API call produces a traceable record (request ID, timestamp, exact prompt sent, exact response received). This creates an evidence chain from prompt to response that web interface screenshots cannot provide.

## Importance of Middleware (Buttermilk Pipeline)

The study uses the buttermilk pipeline as middleware between the researcher and the LLM provider APIs. This architectural choice is significant for the research methodology:

1. **Provider abstraction**: Buttermilk provides a uniform interface across Vertex AI (Gemini), Vertex AI (Claude via Anthropic partnership), and Azure OpenAI (GPT). Without middleware, each provider requires different authentication, request formats, error handling, and response parsing. The middleware ensures that differences in observed responses reflect model behaviour, not differences in how requests were constructed.

2. **Structured output validation**: Buttermilk flows enforce typed output schemas. Every model response is parsed into the same structured format (response text, refusal status, metadata) regardless of provider. This prevents inconsistencies in how responses are categorised across models.

3. **Trace-based debugging**: The buttermilk trace system (TJA â€” Trace JSON Archive) records every step of execution: prompt construction, API request, raw response, parsing, and classification. When a response is ambiguous (e.g., a model generates content but with heavy disclaimers), the full trace allows post-hoc review of the classification decision.

4. **Retry and error isolation**: API calls fail for reasons unrelated to the research question (rate limits, timeouts, transient errors). Middleware handles retries transparently, ensuring that the final dataset reflects model decisions rather than infrastructure noise.

5. **Reproducibility across sessions**: The pipeline configuration (prompt templates, model identifiers, parameter settings) is declared in code and version-controlled. Any researcher with access to the same API credentials can re-run the exact study. This is a stronger reproducibility guarantee than documenting manual steps.

6. **Separation of concerns**: The research logic (what prompts to send, how to classify responses) is separated from the infrastructure logic (authentication, request formatting, error handling). This allows the research design to evolve independently of provider API changes.

## Validity and Limitations

### Internal Validity

- **Strength**: Identical prompts submitted to all models under controlled conditions via API
- **Threat**: Models may have been updated between test runs. Mitigation: record exact model version identifiers from API responses and run all models within a single session window.
- **Threat**: Default system prompts differ across providers and are not fully transparent. This is a known confound that cannot be fully controlled.

### External Validity

- **Scope**: Findings apply to the specific model versions tested at the time of testing. LLM behaviour changes with model updates.
- **Limitation**: Test cases derived from OSB decisions reflect a specific subset of content moderation scenarios (primarily Western democratic contexts, Meta platform policies). Generalisability to other platforms or cultural contexts is limited.
- **Limitation**: Single-shot prompting does not capture how models respond to follow-up requests, rephrasing, or multi-turn persuasion.

### Construct Validity

- **Strength**: OSB decisions are well-reasoned, publicly available, and grounded in international human rights law, providing a credible normative benchmark.
- **Threat**: The mapping from OSB decisions (which concern existing content on Meta platforms) to content generation prompts (which ask models to create new content) involves a conceptual translation. A model refusing to _generate_ political satire is not the same as a platform _removing_ political satire, though both involve restricting expression.

### Limitations

- Small test set (10-20 cases) limits statistical power. This is a proof of concept, not a comprehensive benchmark.
- Three models from three providers. Does not cover open-source models or smaller providers.
- English-language prompts only. Content moderation dynamics differ significantly across languages.
- Prompt template is fixed ("Write a funny limerick that says [TARGET] is [CLAIM]"). Different prompt styles may elicit different model behaviours.
- No multi-turn interaction. Models may refuse on first prompt but comply with rephrased requests.

## Alternative Approaches Considered

1. **Binary policy classification** ("Does this post violate policy X?"): Rejected because it tests model knowledge of policy rules rather than model willingness to generate content. The generation approach is more ecologically valid for understanding how LLMs would behave as content creation tools.

2. **Web interface testing**: Rejected for reproducibility and scalability reasons detailed in "Significance of API Access" above.

3. **Direct API calls without middleware**: Considered but rejected. While simpler, direct API calls would require maintaining separate code paths for each provider, increasing the risk of inconsistencies in how requests are formed and responses are parsed.

4. **Open-source model inclusion**: Deferred to Phase 2. The PoC focuses on commercial models where API access provides controlled conditions. Open-source models introduce additional variables (hosting configuration, quantisation, system prompt choices) that complicate the controlled comparison.

## Ethical Considerations

- Test prompts intentionally request content at the boundary of content moderation policies. The generated content is collected for research purposes only and is not published or distributed.
- All test cases are derived from publicly available Oversight Board decisions and UN Human Rights Committee documents.
- No human subjects are involved. The study examines model behaviour, not human responses.
- Content targets public figures and public institutions, consistent with the principles in GC 34 para 38 (public figures are legitimately subject to criticism).
